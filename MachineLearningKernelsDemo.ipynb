{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top></a>\n",
    "# Machine Learning with Kernels\n",
    "- <a href=#deffunctions>Some needed functions</a>\n",
    "- <a href=#getdata>Obtain and visually inspect Fisher's Iris Data</a>\n",
    "- <a href=#featengine>Feature Engineering</a>\n",
    "- <a href=#logisticregression>Model Fisher's Iris Data with Logistic Regression</a>\n",
    "- <a href=#kernelsvm>Model Fisher's Iris Data with the Kernel Support Vector Machine</a>\n",
    "- <a href=#crossval>Machine Learning Technique - Cross-validation</a>\n",
    "- <a href=#gridsearch>Hyperparameter Tuning - Grid Search</a>\n",
    "- <a href=#featsel>Feature Selection - Combinatorial Enumeration</a>\n",
    "- <a href=#ensemble>Ensemble Modeling</a>\n",
    "\n",
    "- <a href=#bottom>Bottom</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "\n",
    "from sklearn.datasets import load_iris, load_boston, load_diabetes\n",
    "from sklearn.metrics import pairwise, confusion_matrix\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as ply\n",
    "import plotly.tools as plytool\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as plyoff\n",
    "import plotly.subplots as plysub\n",
    "\n",
    "# if using jupyter lab, must have the plotly extension installed: https://github.com/jupyterlab/jupyter-renderers/tree/master/packages/plotly-extension\n",
    "# jupyter labextension install @jupyterlab/plotly-extension\n",
    "# to use plotly offline, need to initialize with a plot; might as well have fun with it\n",
    "plyoff.init_notebook_mode(connected=True)\n",
    "x1 = [1,4,7]; y1 = [7,5,7]\n",
    "x2 = [1,2,3,4,5,6,7]; y2 = [3,2,1,1,1,2,3]\n",
    "plyoff.iplot(go.Figure(data=[go.Scatter({'x':x1, 'y':y1, 'mode':'markers'}),\n",
    "                             go.Scatter({'x':x2, 'y':y2, 'mode':'lines'})],\n",
    "                                        layout=go.Layout(autosize=False,width=500,title=\"Initialization Makes Me Smile<br>(and it's fun to show off a little...)\",\n",
    "                                                         showlegend=False, xaxis={'showgrid':False, 'showticklabels':False},\n",
    "                                                         yaxis={'showgrid':False, 'showticklabels':False})))\n",
    "\n",
    "# if I want to show a df, I want to show *all* of it\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=deffunctions></a>\n",
    "### Some needed functions\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarSubset(p):\n",
    "    \"\"\"\n",
    "    Generate an array of binary indices that can be used for all-subset combinatorial analysis\n",
    "    of a dataset with p features.\n",
    "    ---\n",
    "    Usage: subset_binaries, subset_sizes = VarSubset(p)\n",
    "    ---\n",
    "    p: integer indicating number of features to subset\n",
    "    subset_binaries: (2^p, p) array of all subsets binary indices that can be used to subset\n",
    "        into the presumed original data matrix\n",
    "    subset_sizes: 2^p array indicating number of features in each subset\n",
    "    ---\n",
    "    ex: p = 4; cols = np.arange(p); bins,sizs = QB.VarSubset(p); print(cols[bins[8,:]])\n",
    "    JAH 20060131; JAH 20121018 - ported to python; JAH 20200219 modified to use itertools.combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # check that p is int; could just duck-type it, but if user passes anything else, something is screwed up\n",
    "    if type(p) is not int:\n",
    "        raise ValueError(\"The number of features must be integer: %s\"%VarSubset.__doc__)\n",
    "    \n",
    "    # prepare the output array; we want bool, but have to start with int, so the assignment below works correctly\n",
    "    subBins = np.zeros((2**p,p),dtype=int)\n",
    "    \n",
    "    '''\n",
    "    # slower (order of magnitude at least!) deprecated code\n",
    "    # loop through all subsets :-( getting the binary representations\n",
    "    for cnt in range(1,2**p):\n",
    "        # get binary representation into a list, then put it in the array\n",
    "        tmp = bin(cnt)[2:]\n",
    "        subBins[cnt,(-len(tmp)):] = list(tmp)'''\n",
    "        \n",
    "    # fill in the singleton (and empty) subsets\n",
    "    subBins[1:(p+1),:] = np.eye(p) # shift by 1 to have the empty subset at the beginning\n",
    "    # iterate over combination cardinalities, excluding 1 & p, to fill in the subsets\n",
    "    sttRow = 1+p\n",
    "    for cnt in range(2,p):\n",
    "        # get the cnt-length combinations\n",
    "        combos = np.asarray(list(combinations(range(p), r=cnt)))\n",
    "        c = len(combos)\n",
    "        # create the binary vectors\n",
    "        cmbBins = np.zeros(shape=(c,p))\n",
    "        np.put_along_axis(cmbBins, combos, 1, axis=1)\n",
    "        # store in subBins\n",
    "        subBins[sttRow:(sttRow+c)] = cmbBins\n",
    "        sttRow += c\n",
    "    # fill in the saturated subset\n",
    "    subBins[-1,:] = 1\n",
    "    \n",
    "    # fill in the variable counts\n",
    "    subSize = np.sum(subBins,axis=1)\n",
    "    \n",
    "    # finally sort by variable counts\n",
    "    tmp = np.argsort(subSize)\n",
    "    \n",
    "    return subBins[tmp,:]==1, subSize[tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myConfusionMatrix(y_true, y_pred, labels, names):\n",
    "    # first build the confusion matrix\n",
    "    dat = confusion_matrix(y_true, y_pred, labels)\n",
    "    # now make and return a nice-looking dataframe   \n",
    "    return pd.DataFrame(dat, columns=names, index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=getdata></a>\n",
    "### Obtain and visually inspect Fisher's Iris Data\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "Fisher's Iris data is a famous dataset that has long been used as statisticians for classification (*supervised learning*) and clustering (*unsupervised learning*). The features are four measurements (*sepal_length*, *sepal_width*, *petal_length*, *petal_width*) taken from samples of three different varieties of iris flower (*setosa*, *versicolor*, *virginica*). For classification models, the target is to use the features to predict the variety of iris.\n",
    "<center><img src=\"./irises.jfif\" alt=\"Iris Varieties\" width=\"900\"/></center>\n",
    "\n",
    "\n",
    "Source: Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
    "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
    "     Mathematical Statistics\" (John Wiley, NY, 1950).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' get the data '''\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "features = iris.feature_names\n",
    "varieties = iris.target_names\n",
    "(n,p) = data.shape\n",
    "k = len(np.unique(iris.target))\n",
    "\n",
    "# set colors by variety\n",
    "colors = ['#1f77b4','#ff7f0e','#2ca02c'] # muted blue, safety orange, cooked asparagus green (WTF!)\n",
    "\n",
    "# rename features\n",
    "features = [f[:3]+'_'+f[6:9]for f in features]\n",
    "\n",
    "# make a dataframe\n",
    "cols = ['Class','Variety','Color']; cols.extend(features)\n",
    "irisData = pd.DataFrame(np.c_[labels,[np.nan]*n,[np.nan]*n,data], columns=cols)\n",
    "irisData = irisData.astype({'Class':int})\n",
    "irisData.Variety = irisData.Class.apply(lambda x: varieties[x])\n",
    "irisData.Color = irisData.Class.apply(lambda x: colors[x])\n",
    "\n",
    "# talk\n",
    "mysep='+'*42\n",
    "print('Iris Data: Observations = %d, Features = %d, Classes = %d\\n%s'%(n,p,k,mysep))\n",
    "display(irisData.head())\n",
    "print('Count by Variety\\n%s'%mysep)\n",
    "display(irisData.Variety.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot matrix\n",
    "px.scatter_matrix(irisData, color='Variety', dimensions=features).update_traces(diagonal_visible=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of features by class\n",
    "fig = plysub.make_subplots(rows=2, cols=2, subplot_titles=features, print_grid=False)\n",
    "fig['layout'].update(title = 'Iris Data Feature Box Plots')\n",
    "\n",
    "cells = [(1,1),(1,2),(2,1),(2,2)]\n",
    "for i,feat in enumerate(features):\n",
    "    for var,col in zip(varieties,colors):\n",
    "        thisData = irisData.loc[irisData.Variety==var,feat]\n",
    "        trc = go.Box(y=thisData, name=var, showlegend=(i==0), legendgroup=var,\n",
    "             marker=dict(color=col), boxmean='sd', boxpoints='outliers')\n",
    "        fig.append_trace(trc,cells[i][0],cells[i][1])\n",
    "        \n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=featengine></a>\n",
    "### Feature Engineering\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "**Feature Engineering** is an important part of the data science process, in which new features are computed and added to the dataset, as functions of existing observed features. Examples include pairwise interactions, higher orders, or discretizations. There would be different reasons for engineering features such as these, depending on *domain knowledge* of the *data-generating process*.\n",
    "\n",
    "For example, if modeling oil volume produced from a well in a given month, features may include the $\\text{oil}$ and $\\text{water}$ produced in the previous month. While both these features are likely significant, a third - the $\\text{water cut}=\\frac{\\text{water}}{\\text{water}+\\text{oil}}$ - would also be a useful feature.\n",
    "\n",
    "In the case of Fisher's Iris data, I suspect that interactions between the four input features may be useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all pairwise feature interactions and generate the hierarchy dependency map\n",
    "depends = np.zeros(shape=(p, int(comb(p,2))), dtype=float)\n",
    "for i, pair in enumerate(combinations(range(p),r=2)):\n",
    "    # create the pairwise interaction feature\n",
    "    cross = features[pair[0]]+'_x_'+features[pair[1]]\n",
    "    irisData[cross] = irisData[features[pair[0]]] * irisData[features[pair[1]]]\n",
    "    features.append(cross)\n",
    "    # update the dependency map\n",
    "    depends[pair[0],i] = 1\n",
    "    depends[pair[1],i] = 1    \n",
    "p = len(features) # update the number features\n",
    "display(irisData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=logisticregression></a>\n",
    "### Model Fisher's Iris Data with Logistic Regression\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "In statistics, the **logistic model** is used to model the *probability* of a binary outcome $y_i$ (win vs. lose, or 1 vs. 0). The model assumes that the *log odds* for the binary outcome can be modeled as a linear combination of features in the data:\n",
    "\n",
    "$\\text{log_odds}_i(y_i=1)=\\frac{P\\left(y_i=1\\right)}{1-P\\left(y_i=1\\right)} = b_0+\\sum_{i=1}^pb_ix_i^j,\\ j=1,\\ldots,p$.\n",
    "\n",
    "The **logistic function** (or *logit*) is then used to convert the *log odds* for the binary outcome to the *probability* of the binary class:\n",
    "\n",
    "$P\\left(y_i=1\\right) = \\left(1+e^{-\\text{log_odds}_i}\\right)^{-1}$.\n",
    "\n",
    "A **decision function** is then used to predict, conditional upon the feature values used to compute the *log odds*, the binary outcome:\n",
    "\n",
    "$\\hat{y}_i=\\begin{cases}1 & P\\left(y_i=1\\right)>P_{thresh}\\\\\n",
    "0&P\\left(y_i=1\\right)<=P_{thresh}\\end{cases}$.\n",
    "\n",
    "An optimization algorithm, such as *maximum likelihood* or *gradient descent* is used to optimally estimate the linear coefficients $b_i$, and the decision threshold $P_{thresh}$ can be also be optimized. These parameters are generally optimized to minimize a cost function such as:\n",
    "\n",
    "$\\text{cost}_i=\\sum_{i=1}^n-y_i\\times\\log{\\hat{y}_i} + \\left(1-y_i\\right)\\times\\log{\\left(1-\\hat{y}_i\\right)}$.\n",
    "\n",
    "Logistic regression can be extended to model multinary classes (animal, vegetable, mineral) in multiple ways. If **multinomial logistic regression** is used, the cost function is modified to account for the losses accross all classes. The other common modification is to use **one-vs-rest** logistic regression (OVR), in which a binary logistic model is estimated for each class against all the others (hence the name). For example:\n",
    "- animal vs. vegetable+mineral\n",
    "- vegetable vs. animal+mineral\n",
    "- mineral vs. animal+vegatable\n",
    "\n",
    "When predicting the class for an observation with the OVR method, the predicted probabilities are computed for each class, then normalized to sum to 1. Whichever class is asscociated with the highest normalized probability is the predicted class for that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' fit the model and use it for prediction on the entire dataset '''\n",
    "# prep the data\n",
    "data = irisData[features].values\n",
    "labl = irisData.Class.values\n",
    "# fit & predict\n",
    "LR = LogisticRegression(multi_class='ovr', max_iter=500, n_jobs=-1, random_state=42)\n",
    "LR.fit(X=data, y=labl)\n",
    "pred = LR.predict(X=data)\n",
    "# add prediction to the dataframe\n",
    "irisData['LRPred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the actual model(s) generated\n",
    "modls = ['']*k\n",
    "for var in range(k):\n",
    "    modls[var] = '%s =& %0.2f + '%(varieties[var], LR.intercept_[var]) + ' + '.join(['%0.2f\\\\times %s'%(coef,feat.replace('_','\\\\_')) for feat,coef in zip(features,LR.coef_[var,:])])\n",
    "print('Linear models for predicting the log odds for each class against all the others\\n%s'%mysep)\n",
    "display(Math(r'{}'.format('\\\\begin{align}'+'\\\\\\\\'.join(modls) + '\\\\end{align}')))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' evaluate the model '''\n",
    "# overall correct classification rate\n",
    "classRate = LR.score(X=data, y=labl)\n",
    "print('Logistic Regression Results\\n%s\\nCorrect Classification Rate: %0.2f%%'%(mysep,100*classRate))\n",
    "# confusion matrix\n",
    "confMat = myConfusionMatrix(labl, pred, range(k), varieties)\n",
    "print('Confusion Matrix')\n",
    "display(confMat)\n",
    "print('Prediction Errors')\n",
    "display(irisData[irisData.Class != irisData.LRPred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate the computations for a few observations'''\n",
    "obsDemo = 42 # 42 should be correct, 70 should not\n",
    "\n",
    "# compute class probabilities\n",
    "probs = np.squeeze(LR.predict_proba(X=np.atleast_2d(irisData.loc[obsDemo,features].values)))\n",
    "logOdds = [0]*k\n",
    "rawProbs = [0]*k\n",
    "strs = ['']*k\n",
    "for var in range(k):\n",
    "    coef = LR.coef_[var,:]\n",
    "    # compute\n",
    "    logOdds[var] = LR.intercept_[var] + np.sum(irisData.loc[obsDemo,features]*coef)\n",
    "    rawProbs[var] = 1/(1+math.exp(-logOdds[var]))\n",
    "    # pepare to talk\n",
    "    strs[var] = ' %10s: log odds=%0.2f, probability(norm)=%0.2f(%0.2f)'%(varieties[var],logOdds[var],rawProbs[var],probs[var])\n",
    "# choose the maximum prob as the prediction\n",
    "mx = np.argmax(probs)\n",
    "strs[mx] = strs[mx] + '_P'\n",
    "strs[irisData.loc[obsDemo,'Class']] = strs[irisData.loc[obsDemo,'Class']] + '_A'\n",
    "\n",
    "# share results\n",
    "print('Predicting observation %d'%obsDemo)\n",
    "print('\\n'.join(strs))\n",
    "print('%s\\nActual variety = %s, Predicted variety = %s'%(mysep,irisData.loc[obsDemo,'Variety'], varieties[mx]))\n",
    "print('Prediction was%s correct!'%([' not',''][int(mx==irisData.loc[obsDemo,'Class'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate what logistic regression is doing '''\n",
    "# feature & class to demo\n",
    "thisFeat = 'pet_len'\n",
    "thisVariety = 'setosa'\n",
    "\n",
    "# get data\n",
    "thisClass = np.argwhere(varieties==thisVariety)[0,0]\n",
    "demoData = np.atleast_2d(irisData[thisFeat].values).T\n",
    "demoLabl = irisData.Class.values\n",
    "\n",
    "# fit the logistic model for a single feature\n",
    "LR = LogisticRegression(multi_class='ovr', max_iter=500, n_jobs=-1, random_state=42)\n",
    "LR.fit(X=demoData, y=demoLabl)\n",
    "\n",
    "# obtain result for a single class against the others\n",
    "eqtn = '%0.2f + %0.2f \\\\times %s'%(LR.intercept_[thisClass], LR.coef_[thisClass],thisFeat.replace('_','\\\\_'))\n",
    "logOdds = LR.intercept_[thisClass] + demoData*LR.coef_[thisClass]\n",
    "probs = LR.predict_proba(X=demoData)[:,thisClass]\n",
    "plotDat = pd.DataFrame(np.c_[demoData,logOdds,probs],columns=['data','logodd','prob'])\n",
    "\n",
    "# create the fit lines\n",
    "X = np.squeeze(np.linspace(0,max(demoData),100))\n",
    "Yl = LR.intercept_[thisClass] + X*LR.coef_[thisClass]\n",
    "Yp = LR.predict_proba(X=np.atleast_2d(X).T)[:,thisClass]\n",
    "Yd = np.ones(shape=X.shape)\n",
    "\n",
    "# plot\n",
    "fig = plysub.make_subplots(rows=1, cols=2, print_grid=False,\n",
    "                           subplot_titles=['$log\\\\_odds(%s) = %s$'%(thisVariety,eqtn),\n",
    "                                           '$Probability(%s) = Logit(log\\\\_odds)$'%thisVariety])\n",
    "fig['layout'].update(title = 'Iris %s vs. Other Varieties'%thisVariety, showlegend=True)\n",
    "trcs = [go.Scatter(x=plotDat.data, y=plotDat.logodd, mode='markers', name='Log Odds'),\n",
    "       go.Scatter(x=X,y=Yl, mode='lines', name='Fit Line', line={'color':'black','width':1}, legendgroup='Fit Line')]\n",
    "for t in trcs:\n",
    "    fig.append_trace(t,1,1)\n",
    "trcs = [go.Scatter(x=plotDat.data, y=plotDat.prob, mode='markers', name='Probability'),\n",
    "        go.Scatter(x=X,y=Yp, mode='lines', name='Fit Line', line={'color':'black', 'width':1},legendgroup='Fit Line', showlegend=False),\n",
    "        go.Scatter(x=X,y=Yd*0.4, mode='lines', name='Bands', line={'color':'red', 'width':1}, legendgroup='Bands'),\n",
    "        go.Scatter(x=X,y=Yd*0.5, mode='lines', name='Discriminator', line={'color':'green', 'width':1}),\n",
    "        go.Scatter(x=X,y=Yd*0.6, mode='lines', name='Bands', line={'color':'red', 'width':1}, legendgroup='Bands', showlegend=False)]\n",
    "for t in trcs:\n",
    "    fig.append_trace(t,1,2)\n",
    "\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=kernelsvm></a>\n",
    "### Model Fisher's Iris Data with the Kernel Support Vector Machine\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "#### The Kernel Trick\n",
    "The term *kernel* refers to a **Reproducing Kernel Hilbert Space**, first investigated by the mathematician Aronszajn in 1950. Applying a kernel to some data $X\\in\\mathbb{R}^{n\\times p}$ corresponds to nonlinearly mapping $X$ into a higher dimensional feature space $F$, then taking the dot product of all *pairs of observations* in this space.\n",
    "\n",
    "Instead of mapping the data then computing the dot product, we can simply compute a function $K\\left(X,X\\right)$ which is equivalent; this is called the *kernel trick*. Applying the kernel trick on a dataset of dimensions $n\\times p$ results in an $n\\times n$-sized **Gram matrix** $K$. Kernel-based machine learning techniques, such as kernel support vector machines, apply the specified statistical model to $K$, rather than $X$.\n",
    "\n",
    "While inflating this dimensionality seems counter-intuitive, the benefit we get is that clusters / groups in data can often be better separated in the higher-dimensional kernel space. This is because the elements of the $K$ matrix are all functions of\n",
    "distances between all pairs of observations in $X$.\n",
    "<center><img src=\"./kerneltrick_separation.png\" alt=\"Cluster Separation by the Kernel Trick\" width=\"700\"/></center>\n",
    "\n",
    "There are several kernel functions. The seven most common are:\n",
    "- Polynomial: $\\left(\\gamma x_i^\\prime x_j+c_0\\right)^d$ ($d=1$: linear, $d=2$: quadratic, $d=3$: cubic)\n",
    "- Gaussian RBF: $\\exp\\left(-\\gamma\\left\\Vert x_i-x_j\\right\\Vert^2\\right)$\n",
    "- Sigmoid: $\\tanh\\left(\\gamma x_i^\\prime x_j+c_0\\right)$\n",
    "- Laplace: $\\exp\\left(-\\gamma\\left\\Vert x_i-x_j\\right\\Vert_1\\right)$\n",
    "- Chi-squared: $\\exp\\left(-\\gamma\\sum_i\\left(\\frac{\\left(x_i-x_j\\right)^2}{x_i+x_j}\\right)\\right)$\n",
    "\n",
    "#### Support Vector Machines\n",
    "A binary **Support Vector Machine** (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given data $X$ with known class labels $Y$, the algorithm outputs a hyperplane, estimated using the features in $X$, which optimally separates the observations by class, with the widest separation margin.\n",
    "<center><img src=\"./svm.png\" alt=\"Support Vector Machine\" width=\"250\"/></center>\n",
    "\n",
    "The support vector machine is a generalization of discriminant analysis, in which $m\\le n$ observations are selected to estimate the separating hyperplane defined as\n",
    "$f\\left(x_i\\right) = b_0+\\sum_{j=1}^p b_ix_i^j,\\ j=1,\\ldots, p$.\n",
    "\n",
    "Under both discriminant analysis and support vector machines, the class prediction is defined by\n",
    "\n",
    "$\\hat{y}_i = \\begin{cases}1 & f\\left(x_i\\right) >= 0\\\\\n",
    "0 & f\\left(x_i\\right)<0\\end{cases}$.\n",
    "\n",
    "The intercept and coefficients of the discriminating hyperplane are optimized via quadratic programming:\n",
    "\n",
    "$\\begin{align}\n",
    "\\left(\\mathbf{b}^*,b_0^*\\right)=&\\underset{\\mathbf{b},b,\\xi}{\\min}\\left[\\frac{1}{2}\\mathbf{b}^\\prime\\mathbf{b}+C\\sum_{i=1}^n\\xi_i^d\\right]\\text{, subject to}\\\\\n",
    "&y_i\\left(b_0+\\mathbf{b}^\\prime x_i\\right) \\ge1-\\xi_i\\\\\n",
    "&i=1,\\ldots,m,\\ C>0,\\ \\xi_i\\ge0\n",
    "\\end{align}$\n",
    "\n",
    "When $d=1$, we say the SVM is *L1 soft margin trained*, otherwise, it's *L2 soft margin trained*; $C$ is a regularization constant. This is the primal optimization problem; in many cases, the dual is easier to solve.\n",
    "\n",
    "Note that the mathematical formulation actually requires $y_i\\in[1,-1]$, but this is a trivial modification from the usual $[0,1]$ setup. The binary support vector machine can be extended to multinary classification in the same way as logistic regression.\n",
    "\n",
    "#### Kernel Support Vector Machine\n",
    "Extension to Kernel SVM is straightforward - instead of using the raw data $X$ to estimate the separating hyperplane (and later prediction), the kernelized data $K$ is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' evaluate performance in Feature Space with 4 kernels '''\n",
    "# get the data\n",
    "data = irisData[features].values\n",
    "labl = irisData.Class.values\n",
    "\n",
    "# setup to use the kernels\n",
    "kernels = {'linear':[{'degree':0, 'gamma':'auto', 'coef0':0},None,0.0], # no params needed\n",
    "         'poly':[{'degree':2, 'gamma':1/p, 'coef0':0},None,0.0],        # all params needed\n",
    "         'rbf':[{'degree':0, 'gamma':1/p, 'coef0':0},None,0.0],         # only gamma param needed\n",
    "         'sigmoid':[{'degree':0, 'gamma':1/p, 'coef0':0},None,0.0]}     # degree param neeed\n",
    "kernCnt = len(kernels)\n",
    "\n",
    "# iterate over each kernel to fit & score\n",
    "for kern,val in kernels.items():\n",
    "    # define the estimator\n",
    "    ksvm = SVC(kernel=kern, degree=val[0]['degree'], gamma=val[0]['gamma'], coef0=val[0]['coef0'], random_state=42)\n",
    "    kernels[kern][1] = ksvm\n",
    "    # fit & score\n",
    "    ksvm.fit(X=data, y=labl)\n",
    "    kernels[kern][2] = ksvm.score(X=data, y=labl)\n",
    "    # store predictions\n",
    "    irisData['%sSVMPred'%kern] = ksvm.predict(X=data)\n",
    "    # talk\n",
    "    print('Correct classification rate for %s = %0.2f%%'%(kern,100*val[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' evaluate the model '''\n",
    "thisKernel = 'rbf'\n",
    "# overall correct classification rate\n",
    "classRate = kernels[thisKernel][2]\n",
    "print('%s kernel svm Results\\n%s\\nCorrect Classification Rate: %0.2f%%'%(thisKernel,mysep,100*classRate))\n",
    "# confusion matrix\n",
    "confMat = myConfusionMatrix(labl, irisData['%sSVMPred'%thisKernel].values, range(k), varieties)\n",
    "print('Confusion Matrix')\n",
    "display(confMat)\n",
    "print('Prediction Errors')\n",
    "display(irisData[irisData.Class != irisData['%sSVMPred'%thisKernel]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three all outperformed logistic regression, but there is a possibility that these correct classification rates are all\n",
    "inflated because the model has been *overfit* to the observed data, which was used for both model estimation and model evaluation.  We can fix this with *cross-validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=crossval></a>\n",
    "### Machine Learning Technique - Cross-validation\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "Machine learning models are typically (as long as they are appropriate for the problem and data) very good at learning patterns in the data to which they are fit. This includes the random noise present in that data. A model that fits the data on which it was trained *too well* is said to be **overfit**. Overfitting is a problem because it limits the *generalizability* of a model to data other than that on which it was trained - it hurts subsequent *inference* and *prediction*. Overitting can be overcome by training and tuning a model on one dataset, then testing or evaluating it's performance on another dataset.\n",
    "\n",
    "**Cross-validation** is a process by which a dataset is partitioned into smaller datasets that are used for:\n",
    "- training / tuning a model\n",
    "- testing the performance of a model\n",
    "- evaluating multiple models or for early stopping (this is relatively recent, and optional)\n",
    "\n",
    "Testing a machine learning model on data other than that on which it was trained is beneficial for generalizability because the random noise component of the testing set will typically be different than the random noise in the training set. This is exactly analagous to the situation when new data is subsequently presented to a model. There are several different forms of cross-validation, but no matter which is used, the data is cross-validated multiple times, and the empirical distribution of model results on the testing sets - mean prediction accuracy, for example - is evaluated. **k-fold** is one form of cross-validation.\n",
    "\n",
    "In k-fold cross-validation, a dataset is partitioned (or *folded*) into training / testing sets $k$ times. In each partitioning, a sliding $\\frac{n}{k}$ window of data is defined to be the testing set, with the remaining $n\\times\\left(1-\\frac{1}{k}\\right)$ observations the training set. This can be seen here, for $k=5$.\n",
    "<center><img src=\"./kfoldcrossval_k5.png\" alt=\"5-fold Cross-validation\" width=\"500\"/></center>\n",
    "\n",
    "I tend not to use k-fold cross-validation, instead generally preferring **randomized cross-validation**. Randomized cross-validation does exactly what it sounds like - the modeler selects a proportion $p$ of the data to use for testing, and $n\\times p$ observations are selected uniformly randomly from the dataset as the testing set. The remaining $n\\left(1-p\\right)$ observations become the training set. Because the dataset is partitioned randomly, we generally perform many such random partitions (more than k-fold). Randomized cross-validation with $p=0.20$ may look as shown here (grey arrows indicate testing set).\n",
    "<center><img src=\"./randomcrossval_p20.png\" alt=\"Random Cross-validation\" width=\"400\"/></center>\n",
    "\n",
    "In the images above, the data is shown as data for a binary classification problem, indicated by the color of the markers. This should bring to mind a potential shortcoming of both forms of cross-validation shown here. If the data are being used for classification or clustering, and the proportion of data in any class are too small, it would be possible for a training / testing set to not represent all classes present in a dataset. This would hurt the generalizability which we hope to achieve by using cross-validation.\n",
    "\n",
    "**Stratified cross-validation** solves this - for both k-fold and randomized methods - by folding / randomizing such that the proportion of data in each class in the dataset is preserved in training / testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' overfitting to the observed data is an important concern, so apply cross-validation '''\n",
    "# get the data\n",
    "data = irisData[features].values\n",
    "labl = irisData.Class.values\n",
    "\n",
    "# set up the randomized cross-validator\n",
    "splits = 100\n",
    "testSize = 0.20\n",
    "crossVal = StratifiedShuffleSplit(splits, test_size=testSize, random_state=42)\n",
    "\n",
    "# perform the cross-validation model evaluation; if I only wanted the test score, could have used\n",
    "# the cross_val_score object, but then the 4 kernels would not be evaluated on the same dataset each\n",
    "# time, which could induce bias\n",
    "crossValKernelScores = np.zeros((2,splits, kernCnt),dtype=float)\n",
    "talkFreq = int(0.2*splits)\n",
    "for i,(trn,tst) in enumerate(crossVal.split(data, labl)):\n",
    "    # talk\n",
    "    if (i+1) % talkFreq == 0:\n",
    "        print('Random split %04d of %04d'%(i+1,splits))\n",
    "    # get the training / testing data\n",
    "    trnData = data[trn]; trnLabl = labl[trn]\n",
    "    tstData = data[tst]; tstLabl = labl[tst]\n",
    "    # iterate over each kernel to fit & score\n",
    "    for j,(kern,val) in enumerate(kernels.items()):\n",
    "        # define the estimator\n",
    "        ksvm = SVC(kernel=kern, degree=val[0]['degree'], gamma=val[0]['gamma'], coef0=val[0]['coef0'], random_state=42)\n",
    "        # fit & score\n",
    "        ksvm.fit(X=trnData, y=trnLabl)\n",
    "        crossValKernelScores[0,i,j] = ksvm.score(X=trnData, y=trnLabl)\n",
    "        crossValKernelScores[1,i,j] = ksvm.score(X=tstData, y=tstLabl)\n",
    "    \n",
    "# summarize & talk\n",
    "trnScores = pd.DataFrame(data=crossValKernelScores[0,:,:], columns=kernels.keys())\n",
    "tstScores = pd.DataFrame(data=crossValKernelScores[1,:,:], columns=kernels.keys())\n",
    "scores = trnScores.join(tstScores, how='inner', lsuffix='_trn', rsuffix='_tst').sort_index(axis=1)\n",
    "display(scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of test scores by kernel\n",
    "trc = []\n",
    "for kern in tstScores.columns:\n",
    "    # easier to see differences if sigmoid is skipped, as it sucks for this data\n",
    "    if kern != 'sigmoid':\n",
    "        thisData = irisData.loc[irisData.Variety==var,feat]\n",
    "        trc.append(go.Box(y=tstScores[kern], name=kern, boxmean='sd', boxpoints='outliers'))\n",
    "        \n",
    "plyoff.iplot(go.Figure(data=trc, layout=go.Layout(title='Testing Set Scores')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at median testing set performance, it appears that the best SVM model for this dataset could use either a linear, quadratic polynomial, or Gaussian RBF kernel. But recall that there were some values (gamma, coef0, etc.) that I just set. Perhaps we could identify a truly \"best\" kernel with a different parameterization? We can evaluate this possibility with *hyperparameter tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=gridsearch></a>\n",
    "### Hyperparameter Tuning - Grid Search\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "All machine learning models learn patterns / relationships in an input dataset by setting **parameters**. For example, in the simple *ordinary least squares* (OLS) regression model\n",
    "\n",
    "$y_i = b_0 + b_1x_i + \\varepsilon_i$\n",
    "\n",
    "the intercept ($b_0$) and slope ($b_1$) are model parameters. Parameters are used to encode some information about the data. In the case of OLS, the intercept informs us of the value expected for $y$ when $x=0$; the slope encodes the rate at which $y$ is expected to change for a unit change in $x$.\n",
    "\n",
    "In addition to paramters, most machine learning models emply **hyperparameters**, which define the mathematical behavior of the model and / or it's optimization. Examples of hyperparameters include:\n",
    "- learning rate generally used in gradient descent (a generic optimization method)\n",
    "- regularization used in loss functions; for example L1 = $\\sum_i\\left\\vert y - y_i \\right\\vert$ vs L2 = $\\sum_i\\left(y - y_i\\right)^2$\n",
    "- number of hidden layers and nodes per layer in a neural network\n",
    "- depth of a decision tree\n",
    "- number of clusters in k-means, mixture, or hierarchical clustering models\n",
    "\n",
    "Hyperparameters take precedence over regular parameters in their impact on a model, which is reflected in the name: the prefix *hyper* comes from a greek word (ὑπέρ) meaning \"over\" or \"above\". They could also be seen as conveying information about a model - *metaparameters*, if you will. For example, the primary difference between *ridge regression* and *LASSO regression* is that the former applies a penalty of $\\lambda\\sum b_j^2$ of the regression coefficients, and the latter applies a penalty of $\\lambda\\sum\\left\\vert b_j\\right\\vert$. Hence the difference is that of L2 vs L1 regularization.\n",
    "\n",
    "The optimal value for a hyperparameter (whether a continuous or discrete value) is often dependent upon the data. Hyperparameters are usually optimized by training multiple versions of a model with different values for the hyperparameters, then selecting the values that resulted in the best performance on a held-out testing dataset. This process is called **hyperparameter tuning**, and can be done in a few different ways, including Bayesian Optimization, Gradient Descent, Evolutionary Algorithms, Random Search, or Grid Search.\n",
    "\n",
    "In **Grid Search** hyperparameter tuning, the goal is to search the possible values of a hyperparameter, seeking the best value, as efficiently as possible. This is usually done by generating a grid of values to test via uniform interval sampling from the space of possible values. The narrower the interval, the higher the likelihood of finding the best value. For a hyperparameter that can only take on values from a short enumeration of values, that enumeration is taken as the grid. If multiple hyperparameters are to be tuned simultaneously, the cartesian product of their grids is usually computed, so that all combinations of all grid values may be evaluated. This grid of grids is called the **hyperparameters space**.\n",
    "\n",
    "Iteration with grid search is often a successful way to increase the chance of efficiently finding an optimal set of hyperparameters when the hyperparameters space is very large. This entails iterating over two steps (beginning with the entire space for all hyperparameters):\n",
    "1. grid search the space to find the optimum\n",
    "2. narrow the search space by \"zooming in\" around the previously-found optimum\n",
    "\n",
    "This procedure is akin to **aliasing** in *signal processing*, and is one way to increase the efficiency of grid search.\n",
    "\n",
    "When the number of hyperparameters being tuned simultaneously is large, with many values sampled in their grids, grid search can become too slow, even using aliasing. In that case, **random search** can be used. This procedure does exactly what it sounds like - the space of values is sampled, usually uniformly. With random search, we can often avoid iteration, instead sampling the hyperparameters space with more frequency. The benefit of this is demonstrated here:\n",
    "<center><img src=\"./gridrandomsearch.png\" alt=\"Grid vs Random Search\" width=\"400\"/></center>\n",
    "\n",
    "While random search has been shown to outperform grid search when the hyperparameters space is large, this is at the expense of higher variability in the search results. Also, it is not guaranteed to find the best set of hyperparameters. This is ok, because grid search is also not guaranteed to find the best set of hyperparameters. To be fair, there is little in machine learning that comes with a guarantee.\n",
    "\n",
    "For some continuously-valued hyperparameters, the space might be so large that a regular grid search over the range directly will not very efficently span what is generally considered to be the most important portion of the range. This would be the case for a parameter such as the *learning rate*, which often takes values spanning many orders of magnitude - $[0.0001, 1.0]$, for example. For hyperparameters such as this, a good strategy is to sample the grid in (base 10) log scale. This is demonstrated below by the series marked with <font color='red'>red x</font> and <font color='green'>green x</font>. The plot also demonstrates random search for the learning rate, in the series marked by <font color='red'>red &#9679;</font> and <font color='green'>green &#x25CF;</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate linear- and log-scale random and grid search for a learning rate hyperparameter '''\n",
    "np.random.seed(42)\n",
    "granu = 50\n",
    "\n",
    "# linear sampling\n",
    "learn_rate_0 = np.linspace(0.0001, 1.0, num=granu, endpoint=True)\n",
    "learn_rate_2 = np.random.uniform(0.0001, 1.000001, granu)\n",
    "\n",
    "# log sampling\n",
    "learn_rate_1 = 10**np.linspace(-4, 0, num=granu, endpoint=True)\n",
    "learn_rate_3 = 10**(-4*np.random.rand(granu))\n",
    "\n",
    "# plot\n",
    "lrY = np.ones(shape=learn_rate_0.shape)\n",
    "trcs = [go.Scatter(x=learn_rate_0, y=lrY*1, mode='markers', marker={'symbol':'x','color':'red'}, name='Linear Grid'),\n",
    "        go.Scatter(x=learn_rate_1, y=lrY*2, mode='markers', marker={'symbol':'x','color':'green'}, name='Log Grid'),\n",
    "        go.Scatter(x=learn_rate_2, y=lrY*3, mode='markers', marker={'symbol':'circle','color':'red'}, name='Linear Random'),\n",
    "        go.Scatter(x=learn_rate_3, y=lrY*4, mode='markers', marker={'symbol':'circle','color':'green'}, name='Log Random')]\n",
    "fig = go.Figure(data=trcs, layout = go.Layout(title='Learning Rate Search Values'))\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' perform cross-validated grid search for kernel SVM '''\n",
    "# get the data\n",
    "data = irisData[features].values\n",
    "labl = irisData.Class.values\n",
    "\n",
    "# create the hyperparameter spaces - linear and polynomial kernels collapsed to polynomial with the degree parameter\n",
    "params = [{'kernel':['poly'],'degree':[1,2,3],'gamma':[1/p,1,2],'coef0':[-1,0,1]},\n",
    "          {'kernel':['rbf'],'gamma':[1/p,1,2],'degree':[3],'coef0':[0]},\n",
    "          {'kernel':['sigmoid'],'gamma':[1/p,1,2],'coef0':[-1,0,1],'degree':[3]}]\n",
    "\n",
    "# grid search and prediction\n",
    "GSC = GridSearchCV(estimator=SVC(random_state=42), param_grid=params, cv=crossVal,\n",
    "                   return_train_score=True, n_jobs=-1, verbose=0)\n",
    "GSC.fit(X=data, y=labl)\n",
    "bestKern = GSC.best_params_['kernel']\n",
    "irisData['%sSVMPred_GSC'%bestKern] = GSC.predict(X=data)\n",
    "classRate = GSC.score(X=data, y=labl)\n",
    "\n",
    "# talk\n",
    "print('Best Model Mean Test Set Correct Classification Rate = %0.2f%% (on all data = %0.2f%%)'%(100*GSC.best_score_,100*classRate))\n",
    "for param in GSC.best_params_.keys():\n",
    "    print('\\t%s = %r'%(param,GSC.best_params_[param]))\n",
    "# confusion matrix\n",
    "confMat = myConfusionMatrix(labl, irisData['%sSVMPred_GSC'%bestKern].values, range(k), varieties)\n",
    "print('Confusion Matrix')\n",
    "display(confMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' review the data saved by GSC '''\n",
    "print(GSC.cv_results_.keys())\n",
    "# extract\n",
    "parms = GSC.cv_results_['params']\n",
    "tstMean = GSC.cv_results_['mean_test_score']\n",
    "tstRank = GSC.cv_results_['rank_test_score']\n",
    "trnMean = GSC.cv_results_['mean_train_score']\n",
    "\n",
    "# combine\n",
    "GSCResults = pd.DataFrame(parms).join(pd.DataFrame(np.c_[trnMean,tstMean,tstRank],columns=['Mean Train CCR','Mean Test CCR','Rank']))\n",
    "GSCResults.sort_values(by=['kernel','Rank'], inplace=True)\n",
    "\n",
    "# talk\n",
    "display(GSCResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' review the hyperparameter performance space '''\n",
    "fig = px.scatter_3d(GSCResults, x='gamma', y='degree', z='Mean Test CCR', color='kernel', symbol='coef0')\n",
    "fig.update_layout(title='Hyperparameter Performance Space')\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that the best predictive model may be a linear polynomial kernel SVM with $\\gamma=2$ (though other models seem to perform identically). This is for fitting a model with $p=10$ features - the original four measurements plus the 6 pairwise interactions we engineered. Perhaps a model fit to a subset of these features would perform better, or at least perform similarly while being simpler? We can find out with *feature selection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=featsel></a>\n",
    "### Feature Selection - Combinatorial Enumeration\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>\n",
    "\n",
    "Statistical modelers have been trying models on subsets of features for almost as long as statistical modeling (most of what we call \"machine learning\" is actually statistical modeling) has been around. Perhaps unimaginably, we call the process of selecting a subset of available features **feature selection**. In feature selection, we use some procedure to generate subsets of the existing features, fit a model to them, and evaluate that model to find an optimal subset. The goal of feature selection is generally to balance two considerations: model performance and *model complexity*. It is generally beneficial for a model to be simpler - to use fewer features, for example. We often prefer a simpler model, even if it performs slightly worse than a more complex model. This follows the principle of *occam's razor*.\n",
    "\n",
    "By this point in the knowledge share series, it will come as no surprise when I say that there are several ways to perform feature selection. Consider the oft-followed process during regression modeling of looking at p-values to remove \"unimportant\" features from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate using p-values for feature selection in regressin '''\n",
    "# load the diabeted data - try print(diabetes['DESCR']) for more information\n",
    "diabetes = load_diabetes()\n",
    "dataD = diabetes['data']\n",
    "targD = diabetes['target']\n",
    "featD = diabetes['feature_names']\n",
    "nD,pD = dataD.shape\n",
    "\n",
    "# fit the saturated model and predict\n",
    "lin = LinearRegression(n_jobs=-1)\n",
    "pred = lin.fit(X=dataD, y=targD).predict(X=dataD)\n",
    "\n",
    "# prepare the coefficients and data prepended with the model intercept\n",
    "coefs = lin.coef_.tolist(); coefs.insert(0, lin.intercept_)\n",
    "xwones = np.c_[np.ones(shape=(nD, 1), dtype=float), dataD]\n",
    "featD.insert(0, 'Constant')\n",
    "\n",
    "# compute the feature coefficient statistics: standard error, test stat, p-value, decision\n",
    "alpha = 0.005\n",
    "critVal = stats.t.ppf(1-alpha/2, df=nD-pD-1)\n",
    "MSE = np.sum((targD - pred)**2)/(nD - pD - 1)\n",
    "coefStdErrs = np.sqrt(MSE*(np.linalg.inv(np.dot(xwones.T, xwones)).diagonal()))\n",
    "coefTestStats = coefs / coefStdErrs\n",
    "coefPVals = [2*(1 - stats.t.cdf(abs(ts), n-1)) for ts in coefTestStats]\n",
    "coefDecide = [['Reject','Keep'][int(pval<p)] for pval in coefPVals]\n",
    "\n",
    "# put it all together and talk\n",
    "regStats = pd.DataFrame(index=featD, data={'Coefficient':np.round(coefs, 3), 'Standard Error':np.round(coefStdErrs, 3),\n",
    "                                           'Test Statistic':np.round(coefTestStats, 3), 'P-value':np.round(coefPVals, 3),\n",
    "                                           'Decision':coefDecide}).sort_values(by=['P-value'])\n",
    "print(\"Student's t critical value for alpha=%0.4f = ±%0.2f\"%(alpha, critVal))\n",
    "display(regStats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the fully *saturated model* - with all features included - the modeler would iteratively\n",
    "- remove the last feature from the model\n",
    "- re-fit and score the model\n",
    "\n",
    "until obtaining a model with only \"Keep\" in the last column. There are issues with following this procedure, but that's a conversation for another day.\n",
    "\n",
    "This iterative procedure is generally formalized in what's called *stepwise regression*, and is usually driven by *model criteria* such as $R^2$, the $F-$statistic, AIC, BIC, Mallows' $C_p$, etc. The three main ways of performing stepwise regression are:\n",
    "- *Forward Selection*: Features are added to a model sequentially, in the order of most improvement in the model criterion\n",
    "- *Backward Elimination*: The saturated model is fit, then features are selected for removal sequentially, in the order of least degradation of the model criterion.\n",
    "- *Bidirectional Elimination*: A combination of forward selection & backward elimination, which allows features to both enter & exit the model in each iteration.\n",
    "\n",
    "Of course, the same kind of stepwise procedures could be used for most any machine learning model, as long as there is an approriate model criterion to drive the selection / elimination.\n",
    "\n",
    "Choosing the correct model criterion is of special importance, but is often not done correctly. For example, of the list of criteria in the preceding paragram, only AIC, BIC, and Mallows' $C_p$ penalize a subset regression model for model complexity. The others, which are used most, are all about the model performance - no balance. In the case of a classification model the correct classification rate criterion generally used to assess models would also fail in this aspect.\n",
    "\n",
    "The *genetic algorithm* can be used for a more advanced form of feature selection. Genetic algorithms are a class of general optimization procedures which operate much like natural selection on a set of solutions to a problem (*population* of *individuals*). For the problem of feature selection from a set of $p$ features, each individual is a $p-$length binary string indicating that a feature is in that solution (1) or out of it (0). For our Fisher's Iris Data example, one such solution could be $\\left[1010010000\\right]$, which would indicate selecting only the Sepal Length, Petal Length, and their pairwise interaction feature. As with stepwise methods, a model criterion, or *objective function* is needed to optimize. The GA as a feature selection optimizer is flexible, and can converge to an optimal, or near-optimal set of features relatively quickly. However, it also requires many executions of the objective function, and is quite computationally expensive, so generally most useful for datasets with many features.\n",
    "\n",
    "A simpler way to perform feature selection, that guarantees finding the most optimal subset of features is *combinatorial enumeration* - a.k.a. **brute force**. Combinatorial enumeration does exactly what it sounds like - the model is evauated on the enumeration of all possible combinations of features. This is no mean feat, as the number of ways to combine $p$ features is exponential in $p$; there are $2^p-1$ possible subsets. This is demonstrated below.\n",
    "\n",
    "There is one final consideration when the features in a dataset include compound features such as higher-order ($x_i^2$) or interaction ($x_i\\times x_j$) terms. In this case, we often want to ensure that feature selection selects all simple features involved in any selected compound feature - we want to ensure *hierarchy holds*. For our Fisher's Iris Data example, the argument of hierarchy would insist that the subset $\\left[0000000011\\right]$ (Sepal Width $\\times$ Petal Width & Petal Length $\\times$ Petal Width) is invalid, but $\\left[0111000011\\right]$ (including the three simple features) is valid. While the importance of feature hierarchy in feature selection is, in fact, debated, it is clearly reasonable from the perspective of model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate exponential growth in feature subsets '''\n",
    "ps = [2,3,4,5,10,20,50,100]\n",
    "display(pd.DataFrame(index=ps, data=[2**p - 1 for p in ps], columns=['subsets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' generate all subsets of features '''\n",
    "hierarchy = True\n",
    "\n",
    "# generate all subsets\n",
    "subsets = VarSubset(p)[0][1:] # function I originally wrote in MATLAB in 2006!\n",
    "subsetCount = len(subsets)\n",
    "sizeCounts = pd.Series(data=np.sum(subsets, axis=1)).value_counts().sort_index()\n",
    "# talk\n",
    "print('%d feature subsets, counting by subset size'%subsetCount)\n",
    "display(sizeCounts)\n",
    "\n",
    "# dispose of hierarchy-flouting subsets, maybe\n",
    "valids = [False]*subsetCount\n",
    "if hierarchy:\n",
    "    for i, subset in enumerate(subsets):\n",
    "        # simple vs compound features\n",
    "        simps = subset[:4]\n",
    "        comps = subset[4:]\n",
    "        # are there any compound features selected?\n",
    "        if comps.sum()>0:\n",
    "            # get the simple features expected from all compounds\n",
    "            expect = np.any(depends[:,comps], axis=1)\n",
    "            # check that all are selected\n",
    "            valids[i] = np.all(expect == simps)\n",
    "        else:\n",
    "            # only simples are automatically valid\n",
    "            valids[i] = True\n",
    "    subsets = subsets[valids,:]\n",
    "    subsetCount = len(subsets)\n",
    "    print('%d valid subsets'%subsetCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' perform combinatorial enumeration feature selection with cross-validated grid search for kernel SVM '''\n",
    "splits = 10 # use fewer cross-validation splits to speed things up, or rather, make them less slow\n",
    "# evaluate all subset models\n",
    "fsModels = [None]*subsetCount\n",
    "fsParams = [None]*subsetCount\n",
    "fsScores = [0]*subsetCount\n",
    "talkFreq = max(4, int(0.004*subsetCount))\n",
    "for i, sub in enumerate(subsets):\n",
    "    subStr = ''.join(sub.astype(int).astype(str))\n",
    "    # define the gsc object\n",
    "    fsModels[i] = GridSearchCV(estimator=SVC(random_state=42), param_grid=params,\n",
    "                             cv=StratifiedShuffleSplit(splits, test_size = 0.2, random_state=42),\n",
    "                             n_jobs=-1, verbose=0)    \n",
    "    # cross-validated grid search\n",
    "    fsModels[i].fit(X=data[:,sub], y=labl)\n",
    "    # save the best model & score per subset\n",
    "    fsParams[i] = fsModels[i].best_params_\n",
    "    fsScores[i] = fsModels[i].best_score_ # best Mean Test Set Correct Classification Rate\n",
    "    # get the global best so far\n",
    "    gloBstInd = np.argmax(fsScores)\n",
    "    gloBstScr = fsScores[gloBstInd]\n",
    "    gloBstSubStr = ''.join(subsets[gloBstInd,:].astype(int).astype(str))\n",
    "    # talk\n",
    "    if ((i+1) % talkFreq == 0) | (i == 0) | (i == subsetCount-1):\n",
    "        print('Subset %04d of %04d: %s(%s) = %0.4f; Global Best: %s = %0.4f'%\\\n",
    "              (i+1, subsetCount, fsParams[i]['kernel'], subStr, fsScores[i], gloBstSubStr, gloBstScr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' get final best model results '''\n",
    "# now find the subset with the best score, then get that subset, model, & it's parameters, and perform cross-validation on it\n",
    "best = np.argmax(fsScores)\n",
    "bestSubset = subsets[best,:]\n",
    "bestParams = fsParams[best]\n",
    "bestModel = fsModels[best]\n",
    "bestModelScores = cross_val_score(bestModel, X=data[:,bestSubset], y=labl, cv=crossVal)\n",
    "\n",
    "# talk\n",
    "print('Best Subset of Features: %r\\nBest Kernel SVM Model:'%[f for f, s in zip(features, bestSubset) if s])\n",
    "for param, value in bestParams.items():\n",
    "    print('\\t%s = %r'%(param, value))\n",
    "print('Distribution of Test Set Correct Classification Rates')\n",
    "display(pd.Series(data=bestModelScores, name=bestParams['kernel']).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' review model performance'''\n",
    "# put together the results\n",
    "resultsBySize = pd.DataFrame(data={'Size':np.sum(subsets, axis=1), 'Score':fsScores,\n",
    "                                   'Subset':[[f for f, s in zip(features, sub) if s] for sub in subsets],\n",
    "                                   'Kernel':[p['kernel'] for p in fsParams],\n",
    "                                   'Degree':[p['degree'] for p in fsParams]}).sort_values(by=['Size', 'Score'], ascending=[True,False])\n",
    "# and also filter to get the best by size\n",
    "bestBySize = resultsBySize[~resultsBySize.duplicated(subset=['Size'], keep='first')]\n",
    "\n",
    "# talk\n",
    "display(bestBySize)\n",
    "display(resultsBySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after all this, if we want to use a single model to predict the variety of an iris flower, it appears that we only need to measure the length of the sepal and petal, and the width of the petal, then apply a SVM model, using a linear kernel with coef0 $= -1$ and $\\gamma = 1$. Of course, this is from having only reviewed Logistic Regression and Support Vector Machines (though, to be fair, we did not do the extensive machine learning with Logistic Regression).\n",
    "\n",
    "The reason why there are different machine learning models is that they have their own strengths & weaknesses. There is no model that will always be best for all datasets that can ever exist (though a Deep Learning Neural Networks may someday be that). Can we create a model that combines multiple models together, uses the strengths of some to combat the weaknesses of others? Yes we can; this is called *ensemble modeling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=ensemble></a>\n",
    "### Ensemble Modeling\n",
    "<a href=#top>Go to top</a> - <a href=#bottom>Go to bottom</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've all heard the phrase \"less is more\", but that is not always the case in machine learning. Founded on the idea that the only thing better than a machine learning model is two machine learning models (gratuitous *Water Boy* reference!), the **ensemble model**, which is literally an ensemble of models, proves this. I began modeling with ensemble models in the early 2000's, having never heard of the term. At the time, I was developing daily trading model for *index mutual funds*. We reasoned that we could make more robust and more accurate predictions on the direction of the market if we used a set of models that diversified by instrument, methodology, source, and frequency. An important component of an ensemble model is the *voting mechanism* - this defines how you handle when the component models disagree. For trading, I developed an allocation system that allocated a fixed number of votes among the component models based on the recent performance of their symbols. As compared to any of the component models, the resultant ensemble model made higher returns, spent less time in drawdowns, and experienced lower volatility over many years of bull, bear, and whipsaw markets. This is called *market neutrality*.\n",
    "\n",
    "Perhaps the simplest way to create a machine learning ensemble model is to simply fit $M$ models to a dataset, and combine their predictions. In the case of predicting a continuous response, the voting mechanism could simply average the predicted values. For classification, we could pick the most often predicted class, choose the class predicted by the model that had the highest predicted probability, or combine & rebalance all models' class probabilities. For example, we could fit both the Logistic Regression and Kernel SVM models we've already seen to the Fisher Iris Data to predict the flower varieties. In my prior work on predicting North Sea field production volumes, I created an ensemble model that averaged together *elastic net*, *gradient boosting*, and *adaptive boosting* regression models. This was then treated as a single model, and ensembled together with the WM forecasts using a static .75 / .25 weighted average.\n",
    "\n",
    "In machine learning, there are three major ideas behind ensemble models: *bagging*, *boosting*, and *stacking*. Bagging, originally a contraction of the term *bootstrap aggregating*, works by combining many models together, in which each one is fit to a *bootstrapped* dataset. Bootstrapping is the process of randomly selecting - with replacement - a subset of observations from a dataset. In addition to this, bagging can also refer to *attribute bagging* or *feature bagging*, in which each model is fit to a randomly-selected subset of the features. The *random forest* model typically uses both observation and feature bagging. Boosting is a complex and precisely-defined mathematical mechanism that typically works by fitting a series of models to a dataset, iterating over two steps:\n",
    "- fit a *weak learner* model\n",
    "- reweight observations, giving higher weighting to those that the model got most wrong.\n",
    "\n",
    "Perhaps a stacked ensemble model could be almost thought of as *psuedo-boosting*, though there's no weighting of observations. There are different ways to create a stacked ensemble model, but one would be to use multiple models create predictions, then use those predictions as features for another model. This could include, for example, fitting a regression model to some data, then fitting a second regression model to the residuals to predict the same response.\n",
    "\n",
    "Ensemble models are used for multiple reasons:\n",
    "- allow the strengths of some models to overcome the weakness of others\n",
    "- reduce model variance\n",
    "- avoid overfitting\n",
    "- reduce correlation among models\n",
    "- reduce bias\n",
    "\n",
    "The only downsides to ensemble models that I can think of is the additional computational time, additional complexity, and possible difficulty interpreting / explaining results.\n",
    "\n",
    "Scikit-learn documentation currently lists almost 20 [ensemble methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate voting classifier '''\n",
    "# component models\n",
    "models = ['Ensemble','Logistic Regression', 'Kernel SVM']\n",
    "LGRG = LogisticRegression(multi_class='ovr', max_iter=500, n_jobs=-1, random_state=42)\n",
    "KSVM = SVC(kernel=GSC.best_params_['kernel'], degree=GSC.best_params_['degree'],\n",
    "           gamma=GSC.best_params_['gamma'], coef0=GSC.best_params_['coef0'], random_state=42)\n",
    "\n",
    "# setup the voting classifier\n",
    "vot = 'hard' # hard = uses predicted class labels for majority voting; soft = uses predicted probabilities\n",
    "votClas = VotingClassifier(estimators=[('Logistic Regression', LGRG), ('Kernel SVM', KSVM)], n_jobs=-1)\n",
    "\n",
    "# perform the cross-validation model evaluation\n",
    "crossValScores = np.zeros((2, splits, 3), dtype=float)\n",
    "talkFreq = int(0.2*splits)\n",
    "for i,(trn, tst) in enumerate(crossVal.split(data, labl)):\n",
    "    # talk\n",
    "    if (i+1) % talkFreq == 0:\n",
    "        print('Random split %04d of %04d'%(i+1,splits))\n",
    "    # get the training / testing data\n",
    "    trnData = data[trn]; trnLabl = labl[trn]\n",
    "    tstData = data[tst]; tstLabl = labl[tst]\n",
    "    # fit & score\n",
    "    votClas.fit(X=trnData, y=trnLabl)\n",
    "    crossValScores[0,i,0] = votClas.score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,0] = votClas.score(X=tstData, y=tstLabl)\n",
    "    # do the same for the component models\n",
    "    crossValScores[0,i,1] = votClas.named_estimators_['Logistic Regression'].score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,1] = votClas.named_estimators_['Logistic Regression'].score(X=tstData, y=tstLabl)\n",
    "    crossValScores[0,i,2] = votClas.named_estimators_['Kernel SVM'].score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,2] = votClas.named_estimators_['Kernel SVM'].score(X=tstData, y=tstLabl)\n",
    "    \n",
    "# summarize & talk\n",
    "trnScores = pd.DataFrame(data=crossValScores[0,:,:], columns=models)\n",
    "tstScores = pd.DataFrame(data=crossValScores[1,:,:], columns=models)\n",
    "scores = trnScores.join(tstScores, how='inner', lsuffix='_trn', rsuffix='_tst').sort_index(axis=1)\n",
    "display(scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate stacking classifier '''\n",
    "# setup the stacked classifier (using the same models as the voting classifier)\n",
    "crossValInput = 5 # this indicates 5-fold stratified cv\n",
    "passThrough = True # true = final trained on X + predictions, false = final trained only on predictions\n",
    "finalEstim = DecisionTreeClassifier(random_state=42)\n",
    "stkClas = StackingClassifier(estimators=[('Logistic Regression', LGRG), ('Kernel SVM', KSVM)],\n",
    "                             cv=crossValInput, passthrough=passThrough, final_estimator=finalEstim,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "# perform the cross-validation model evaluation\n",
    "crossValScores = np.zeros((2, splits, 3), dtype=float)\n",
    "talkFreq = int(0.2*splits)\n",
    "for i,(trn, tst) in enumerate(crossVal.split(data, labl)):\n",
    "    # talk\n",
    "    if (i+1) % talkFreq == 0:\n",
    "        print('Random split %04d of %04d'%(i+1,splits))\n",
    "    # get the training / testing data\n",
    "    trnData = data[trn]; trnLabl = labl[trn]\n",
    "    tstData = data[tst]; tstLabl = labl[tst]\n",
    "    # fit & score\n",
    "    stkClas.fit(X=trnData, y=trnLabl)\n",
    "    crossValScores[0,i,0] = stkClas.score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,0] = stkClas.score(X=tstData, y=tstLabl)\n",
    "    # do the same for the component models\n",
    "    crossValScores[0,i,1] = stkClas.named_estimators_['Logistic Regression'].score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,1] = stkClas.named_estimators_['Logistic Regression'].score(X=tstData, y=tstLabl)\n",
    "    crossValScores[0,i,2] = stkClas.named_estimators_['Kernel SVM'].score(X=trnData, y=trnLabl)\n",
    "    crossValScores[1,i,2] = stkClas.named_estimators_['Kernel SVM'].score(X=tstData, y=tstLabl)\n",
    "    \n",
    "# summarize & talk\n",
    "trnScores = pd.DataFrame(data=crossValScores[0,:,:], columns=models)\n",
    "tstScores = pd.DataFrame(data=crossValScores[1,:,:], columns=models)\n",
    "scores = trnScores.join(tstScores, how='inner', lsuffix='_trn', rsuffix='_tst').sort_index(axis=1)\n",
    "display(scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' plot the decision tree & show feature importances from the last stacking classifier '''\n",
    "# need to create a list of features, I *think* with passthrough=True that the features in order are\n",
    "# Logistic Regression class probs, Kernel SVM class probs, original features\n",
    "feats = ['_'.join(mv) for mv in product(models[1:],varieties)]\n",
    "feats.extend(features)\n",
    "\n",
    "# show the feature importances\n",
    "featImports = [(f, i) for f, i in zip(feats, stkClas.final_estimator_.feature_importances_)]\n",
    "featImports.sort(key=lambda x:x[1])\n",
    "for (f, i) in featImports[::-1]:\n",
    "    print('%s = %0.2f'%(f,i))\n",
    "\n",
    "# plot\n",
    "_ = plot_tree(stkClas.final_estimator_, feature_names=feats, class_names=varieties, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' demonstrate random forest '''\n",
    "# setup the random forest classifier\n",
    "numberTrees = 100\n",
    "RF = RandomForestClassifier(n_estimators=numberTrees, random_state=42, n_jobs=-1)\n",
    "\n",
    "# perform the cross-validation model evaluation\n",
    "# cross-validation is probably not strictly required, since each decision tree sees a different sample\n",
    "crossValScores = np.zeros((splits, 2), dtype=float )\n",
    "talkFreq = int(0.2*splits)\n",
    "for i,(trn, tst) in enumerate(crossVal.split(data, labl)):\n",
    "    # talk\n",
    "    if (i+1) % talkFreq == 0:\n",
    "        print('Random split %04d of %04d'%(i+1,splits))\n",
    "    # get the training / testing data\n",
    "    trnData = data[trn]; trnLabl = labl[trn]\n",
    "    tstData = data[tst]; tstLabl = labl[tst]\n",
    "    # fit & score\n",
    "    RF.fit(X=trnData, y=trnLabl)\n",
    "    crossValScores[i,0] = RF.score(X=trnData, y=trnLabl)\n",
    "    crossValScores[i,1] = RF.score(X=tstData, y=tstLabl)\n",
    "\n",
    "# summarize & talk\n",
    "trnScores = pd.DataFrame(data=crossValScores[:,0], columns=['Random Forest'])\n",
    "tstScores = pd.DataFrame(data=crossValScores[:,1], columns=['Random Forest'])\n",
    "scores = trnScores.join(tstScores, how='inner', lsuffix='_trn', rsuffix='_tst').sort_index(axis=1)\n",
    "display(scores.describe())\n",
    "# final random forest performance\n",
    "print('Correct Classification Rate: %0.3f%%'%(100*crossValScores[-1,1]))\n",
    "# final random forest overall feature importances\n",
    "featImports = [(f, i) for f, i in zip(features, RF.feature_importances_)]\n",
    "featImports.sort(key=lambda x:x[1])\n",
    "for (f, i) in featImports[::-1]:\n",
    "    print('%s = %0.2f'%(f,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' get some information from the last random forest '''\n",
    "# choose the best tree, based on the last testing set\n",
    "treeScores = [atree.score(X=tstData, y=tstLabl) for atree in RF.estimators_]\n",
    "bestTree = np.argsort(treeScores)[-1]    # just reusing variable\n",
    "bestTreeScore = treeScores[bestTree]\n",
    "bestTree = RF.estimators_[bestTree]\n",
    "print('Best Tree Correct Classification Rate: %0.3f%%'%(100*bestTreeScore))\n",
    "\n",
    "# show the feature importances\n",
    "featImports = [(f, i) for f, i in zip(features, bestTree.feature_importances_)]\n",
    "featImports.sort(key=lambda x:x[1])\n",
    "for (f, i) in featImports[::-1]:\n",
    "    print('%s = %0.2f'%(f,i))\n",
    "\n",
    "# plot it\n",
    "_ = plot_tree(bestTree, feature_names=features, class_names=varieties, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bottom></a>Done! <a href=#top>Go to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
